{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyO79oXrx4/pdc10mVnh3aXr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"dc876c04ed434180811180f6406981a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e483afc5069b403ab35c540749250f9f","IPY_MODEL_9f87d1c5e34d48c78f973e0dbad61c57","IPY_MODEL_d0a89c6a88094e179027ce64b6c7f1f1"],"layout":"IPY_MODEL_502632d6ef274b9cab5f2d05321e0331"}},"e483afc5069b403ab35c540749250f9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88a4d478087d448c824be55711204a2f","placeholder":"​","style":"IPY_MODEL_f7fb06977c624a998e36525bf6f01d13","value":"config.json: 100%"}},"9f87d1c5e34d48c78f973e0dbad61c57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72c7beaf187249ccac9668888237c114","max":553,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74eee09519794e7cbec7d68ba09bf2be","value":553}},"d0a89c6a88094e179027ce64b6c7f1f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0c7da1e2dc0432999925afed0ee37ab","placeholder":"​","style":"IPY_MODEL_728ae276474f4dcc9e31a5fdd2379ea4","value":" 553/553 [00:00&lt;00:00, 47.0kB/s]"}},"502632d6ef274b9cab5f2d05321e0331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88a4d478087d448c824be55711204a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7fb06977c624a998e36525bf6f01d13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72c7beaf187249ccac9668888237c114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74eee09519794e7cbec7d68ba09bf2be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0c7da1e2dc0432999925afed0ee37ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"728ae276474f4dcc9e31a5fdd2379ea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dbe8ab197634b078a687561a35e84c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54ce8b5be01c4e0991aba2cb2435b067","IPY_MODEL_1daa73c5fdf246a98a1aff115bb0fb9a","IPY_MODEL_917c178312e34e17902f71f625f0854e"],"layout":"IPY_MODEL_d666500edddf41588b6a13657c2b2d43"}},"54ce8b5be01c4e0991aba2cb2435b067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88490801c81a45a3b829858ecbf18ab2","placeholder":"​","style":"IPY_MODEL_c779df94fd3f470d8bc9fd139ca41a6f","value":"pytorch_model.bin: 100%"}},"1daa73c5fdf246a98a1aff115bb0fb9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6675e2fe93a04466b9435824f60ca18c","max":1200794589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef0afb7bd97b426e880d1a45e719ae40","value":1200794589}},"917c178312e34e17902f71f625f0854e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c733d47148aa48c48ea01803a12b4250","placeholder":"​","style":"IPY_MODEL_f742036a413b4a4d87a833c404446af6","value":" 1.20G/1.20G [00:07&lt;00:00, 171MB/s]"}},"d666500edddf41588b6a13657c2b2d43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88490801c81a45a3b829858ecbf18ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c779df94fd3f470d8bc9fd139ca41a6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6675e2fe93a04466b9435824f60ca18c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef0afb7bd97b426e880d1a45e719ae40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c733d47148aa48c48ea01803a12b4250":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f742036a413b4a4d87a833c404446af6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21fedb42e55f49439e0fbf0627fd1e9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_187dc0e6cc90460fb62f043a6962918d","IPY_MODEL_b996b71e475d47479943375b63214432","IPY_MODEL_20d44fe8f82f4d3a91e3a8089f64ec75"],"layout":"IPY_MODEL_13893e4f51e14740ad8808c21bc21883"}},"187dc0e6cc90460fb62f043a6962918d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15c5c80e10e34b57ae5c67b4647f866b","placeholder":"​","style":"IPY_MODEL_a04bffca1a2347c083a3125bbd07e8c8","value":"generation_config.json: 100%"}},"b996b71e475d47479943375b63214432":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22af94e3d48c4d85b0e791eee9561900","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_524ca38d989b4af19bda142e56cc2bd5","value":147}},"20d44fe8f82f4d3a91e3a8089f64ec75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eba869968834e2198a8569f9c2dc077","placeholder":"​","style":"IPY_MODEL_4b1bbd55d27a44669e4ea04fe439e362","value":" 147/147 [00:00&lt;00:00, 12.2kB/s]"}},"13893e4f51e14740ad8808c21bc21883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15c5c80e10e34b57ae5c67b4647f866b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a04bffca1a2347c083a3125bbd07e8c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22af94e3d48c4d85b0e791eee9561900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"524ca38d989b4af19bda142e56cc2bd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0eba869968834e2198a8569f9c2dc077":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b1bbd55d27a44669e4ea04fe439e362":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D26RXHIKxqUj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715355217353,"user_tz":240,"elapsed":99773,"user":{"displayName":"cs685Translation","userId":"00978543179663547915"}},"outputId":"0774e06d-4381-4b9c-f260-4a4f718b281e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Collecting peft\n","  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.40.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: peft\n","Successfully installed peft-0.10.0\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Installing collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.43.1\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.2\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"]}],"source":[" !pip install transformers[torch]\n","!pip install peft\n","!pip install bitsandbytes\n","!pip install sacrebleu\n","!pip install matplotlib\n","!pip install nltk\n","!pip install accelerate -q"]},{"cell_type":"code","source":["from transformers import MT5ForConditionalGeneration, AutoTokenizer\n","import pandas as pd\n","import torch\n","import numpy as np\n","from accelerate import Accelerator\n","import os,shutil\n","\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","torch.cuda.empty_cache()"],"metadata":{"id":"Lii2omiUB68Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzt3Wvab_cWr","executionInfo":{"status":"ok","timestamp":1715355254153,"user_tz":240,"elapsed":14644,"user":{"displayName":"cs685Translation","userId":"00978543179663547915"}},"outputId":"2e56d6ab-8ef9-4823-90fc-35f4e4941fb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["model_name =\"./drive/MyDrive/mt5-qlora-200k\""],"metadata":{"id":"_QCUdfpmIMgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ebfd5c33-ef6d-4224-bc87-57c3ebcc828a","colab":{"base_uri":"https://localhost:8080/","height":289,"referenced_widgets":["dc876c04ed434180811180f6406981a3","e483afc5069b403ab35c540749250f9f","9f87d1c5e34d48c78f973e0dbad61c57","d0a89c6a88094e179027ce64b6c7f1f1","502632d6ef274b9cab5f2d05321e0331","88a4d478087d448c824be55711204a2f","f7fb06977c624a998e36525bf6f01d13","72c7beaf187249ccac9668888237c114","74eee09519794e7cbec7d68ba09bf2be","f0c7da1e2dc0432999925afed0ee37ab","728ae276474f4dcc9e31a5fdd2379ea4","9dbe8ab197634b078a687561a35e84c5","54ce8b5be01c4e0991aba2cb2435b067","1daa73c5fdf246a98a1aff115bb0fb9a","917c178312e34e17902f71f625f0854e","d666500edddf41588b6a13657c2b2d43","88490801c81a45a3b829858ecbf18ab2","c779df94fd3f470d8bc9fd139ca41a6f","6675e2fe93a04466b9435824f60ca18c","ef0afb7bd97b426e880d1a45e719ae40","c733d47148aa48c48ea01803a12b4250","f742036a413b4a4d87a833c404446af6","21fedb42e55f49439e0fbf0627fd1e9b","187dc0e6cc90460fb62f043a6962918d","b996b71e475d47479943375b63214432","20d44fe8f82f4d3a91e3a8089f64ec75","13893e4f51e14740ad8808c21bc21883","15c5c80e10e34b57ae5c67b4647f866b","a04bffca1a2347c083a3125bbd07e8c8","22af94e3d48c4d85b0e791eee9561900","524ca38d989b4af19bda142e56cc2bd5","0eba869968834e2198a8569f9c2dc077","4b1bbd55d27a44669e4ea04fe439e362"]},"executionInfo":{"status":"ok","timestamp":1715355278104,"user_tz":240,"elapsed":19402,"user":{"displayName":"cs685Translation","userId":"00978543179663547915"}},"outputId":"21afe579-54a9-4e1e-d99a-4cb306acef5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc876c04ed434180811180f6406981a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dbe8ab197634b078a687561a35e84c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fedb42e55f49439e0fbf0627fd1e9b"}},"metadata":{}}],"source":["# Load the Trained Model\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = MT5ForConditionalGeneration.from_pretrained(model_name)\n","accelerator = Accelerator()\n","model = accelerator.prepare(model)"]},{"cell_type":"code","source":["model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTej_2Mi9nx8","executionInfo":{"status":"ok","timestamp":1715355282365,"user_tz":240,"elapsed":168,"user":{"displayName":"cs685Translation","userId":"00978543179663547915"}},"outputId":"3c2f1fbc-724b-47a0-a78e-90bb4c37cb38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MT5ForConditionalGeneration(\n","  (shared): Embedding(250112, 512)\n","  (encoder): MT5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): MT5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): MT5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerCrossAttention(\n","            (EncDecAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerCrossAttention(\n","            (EncDecAttention): MT5Attention(\n","              (q): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): lora.Linear(\n","                (base_layer): Linear(in_features=512, out_features=384, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=384, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): MT5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"7baf7b88-52d9-4a4b-984b-fccfa354589f"},"outputs":[],"source":["def encode_str(text, tokenizer,):\n","    input_ids = tokenizer.encode(\n","      text=text,\n","      return_tensors = 'pt',\n","      padding = 'max_length',\n","      truncation = True,)\n","    return input_ids[0]\n","\n","def random_model_test(model,tokenizer, df_validation,n=10,verbose=True):\n","    random_test= df_validation.sample(n=n)\n","    for i in range(len(random_test)):\n","        en_test_data = random_test.iloc[i].en\n","        fr_test_data = random_test.iloc[i].fr\n","        en = encode_str(en_test_data,tokenizer).unsqueeze(0).cuda()\n","        res  = model.generate(en)\n","        res_decoded= tokenizer.decode(res[0],skip_special_tokens=True)\n","        if verbose:\n","            print('English Sentence:')\n","            print(en_test_data)\n","            print('French Sentence:')\n","            print(fr_test_data)\n","            print('Model Output:')\n","            print(res_decoded)\n","            print('--------\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"a4d4db21-36ed-4904-99f0-7d8a3f4b4639"},"outputs":[],"source":["df= pd.read_csv('test.csv',nrows=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"1a0b383d-0781-4114-9afb-3e747d4aba7f","outputId":"e27e59a7-f4ea-44ac-b5e2-875d6cc70a58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715355305793,"user_tz":240,"elapsed":4479,"user":{"displayName":"cs685Translation","userId":"00978543179663547915"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["English Sentence:\n","Some donors are planning to go beyond their initial pledges (US, Algeria).\n","French Sentence:\n","Certains donateurs prévoient d'aller au-delà de leurs engagements initiaux (États-Unis, Algérie).\n","Model Output:\n","<extra_id_0>s ont été planning à l’avance (US, Algeria).\n","--------\n","\n","English Sentence:\n","Other remedial work suggested in the Draft Report be undertaken.  17.\n","French Sentence:\n","Que d'autres travaux de restauration proposés dans le rapport provisoire soient entrepris.  17.\n","Model Output:\n","<extra_id_0> à l’étude de la Draft Report a été arrêt\n","--------\n","\n","English Sentence:\n","As a result, section 7 of the Charter is inapplicable in the present case.\n","French Sentence:\n","Par conséquent, l'article 7 de la Charte est inapplicable dans la présente cause.\n","Model Output:\n","<extra_id_0> a été l’article 7 de la Charter est inapplicable en\n","--------\n","\n","English Sentence:\n","Mexico’s ratification affirms the overwhelming support for the International Criminal Court.\n","French Sentence:\n","La ratification du Mexique confirme le soutien massif à la Cour pénale internationale.\n","Model Output:\n","<extra_id_0> affirme la sécurité de la justice internationale.\n","--------\n","\n","English Sentence:\n","This initiative will enhance the bilateral relationship with France.\n","French Sentence:\n","Cette initiative permettra d'approfondir les relations bilatérales entre le Canada et la France.\n","Model Output:\n","<extra_id_0> a augmenter la relation avec France.\n","--------\n","\n","English Sentence:\n","Thirdly, it makes available some of their surviving letters and selections from their books.\n","French Sentence:\n","Enfin, il diffuse quelques-unes de leurs lettres restantes et des extraits de leurs livres.\n","Model Output:\n","<extra_id_0>, l'attribution des listes et des selections de leurs manuscri\n","--------\n","\n","English Sentence:\n","Announcement Dramamuse presents Once upon a Pizzeria\n","French Sentence:\n","Annonce Dramamuse vous présente Il était une fois une pizzeria\n","Model Output:\n","<extra_id_0> présente l’invitation Dramamuse présente Onceupon a Pizzeria.\n","--------\n","\n","English Sentence:\n","What is generally in the federal government benefits package?\n","French Sentence:\n","• un régime de pension; • une assurance-vie;\n","Model Output:\n","<extra_id_0> Qué l’aucun a été à la contribution de la\n","--------\n","\n","English Sentence:\n","No user fee table included in Departmental Performance Report.\n","French Sentence:\n","Le Rapport ministériel sur le rendement ne comporte aucun tableau de frais d’utilisation.\n","Model Output:\n","<extra_id_0> Prix à l’article l’article l’article l\n","--------\n","\n","English Sentence:\n","Both the prosecution and the applicant appealed.\n","French Sentence:\n","A la date de l’adoption de l’arrêt de la Grande\n","Model Output:\n","<extra_id_0> et le applicant a été refusées.\n","--------\n","\n"]}],"source":["random_model_test(model,tokenizer, df,n=10,verbose=True)"]},{"cell_type":"code","source":[],"metadata":{"id":"KF_LRAHJjqr0"},"execution_count":null,"outputs":[]}]}